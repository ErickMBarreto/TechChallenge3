{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcNiQFWDCJlNzDpID3jrAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErickMBarreto/TechChallenge3/blob/main/TechChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fiap Tech Challenge 3\n",
        "\n",
        "**Apresentação**\n",
        "\n",
        "Este notebook foi elaborado para o Tech Challenge, uma atividade que consolida e utiliza os conceitos aprendidos durante o curso. A meta principal é executar o fine-tuning de um foundation model\n",
        " (neste projeto, o LLaMA-3 8B, em sua versão quantizada de 4 bits com a biblioteca Unsloth), empregando o dataset AmazonTitles-1.3MM.\n",
        "\n",
        "As etapas do processo são:\n",
        "\n",
        "1º **Realizar o pré-processamento do dataset** a partir do arquivo trn.json, que inclui os campos title (nome do produto) e content (descrição).\n",
        "\n",
        "2º **Estruturar os dados em prompts no estilo Alpaca**, organizados com Instruction, Input e Response, para viabilizar o treinamento supervisionado.\n",
        "\n",
        "3º **Aplicar o fine-tuning ao modelo base utilizando a metodologia LoRA** (Low-Rank Adaptation), que permite otimizar o treinamento mesmo com recursos computacionais restritos.\n",
        "\n",
        "4º **Analisar a performance do modelo antes e depois do ajuste**, medindo sua habilidade de criar descrições lógicas com base nos títulos dos produtos.\n",
        "\n",
        "5º **Preparar o modelo ajustado para a inferência**, de modo que ele possa responder às consultas do usuário, fornecendo a descrição apropriada conforme o conhecimento adquirido no fine-tuning.\n",
        "\n",
        "Conectando ao Google Drive\n",
        "\n",
        "Estabelecendo a conexão com o Google Drive, onde os arquivos necessários para o treinamento estão armazenados."
      ],
      "metadata": {
        "id": "OJEo4jDWp2A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ABoThDqpo92M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instalando a biblioteca para acelerar o treino**\n",
        "\n",
        "Instala também o transformers e o dataset"
      ],
      "metadata": {
        "id": "e2FwoT6Ypyaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new]\" -U\n",
        "!pip install --no--deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "BQkOCFlsp9Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo Llama 3"
      ],
      "metadata": {
        "id": "3vUrt5Fit2eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_id = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_id,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")"
      ],
      "metadata": {
        "id": "UGnfiixRt4jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função de Inferência para Geração de Respostas\n",
        "\n",
        "Este trecho define a função run_chat, responsável por realizar a inferência do modelo a partir de um prompt fornecido pelo usuário:\n",
        "\n",
        "1. Tokenização → O texto de entrada (prompt) é convertido em tensores pelo tokenizer e enviado para a GPU (cuda).\n",
        "\n",
        "2. Geração de texto → O modelo recebe os tokens e gera uma continuação da sequência, limitada a max_new_tokens=64.\n",
        "\n",
        "3. Decodificação → A saída numérica é convertida de volta para texto legível com tokenizer.batch_decode.\n",
        "\n",
        "Essa função encapsula o fluxo completo de interação com o modelo, permitindo que o usuário teste diferentes prompts e visualize as respostas geradas de forma simples e direta."
      ],
      "metadata": {
        "id": "jdJoQOy9v9DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chat(prompt, tokenizer, model):\n",
        "  inputs = tokenizer([prompt], return_tensors='pt').to('cuda')\n",
        "  outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=64,\n",
        "      use_cache = True\n",
        "  )\n",
        "  return tokenizer.batch_decode(outputs)\n"
      ],
      "metadata": {
        "id": "d4jy4XJhwbDF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferência com Prompt Personalizado\n",
        "\n",
        "Este trecho executa a função run_chat, enviando ao modelo um prompt que solicita detalhes sobre o livro Harry Potter and the Philosopher's Stone.\n",
        "\n",
        "O texto é processado pelo tokenizer e passado ao modelo já treinado.\n",
        "\n",
        "O modelo gera uma resposta baseada em seu conhecimento e no fine-tuning realizado.\n",
        "\n",
        "A saída é retornada ao usuário em linguagem natural, simulando uma consulta real sobre um livro técnico de desenvolvimento de software.\n",
        "\n",
        "Esse exemplo serve como validação prática do funcionamento do pipeline de inferência criado no notebook."
      ],
      "metadata": {
        "id": "Rg3_FXSsxfwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_chat(\"Could you provide details about the book Clean Architeture: A Craftsman's Guide to Software Structure and Design?\", tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "v9EZgwq_xljj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pré-processamento e Geração do Dataset para Fine-Tuning\n",
        "\n",
        "Esse código faz o pré-processamento de dados para criar um conjunto de dados em formato JSON, ideal para o treinamento e ajuste fino (fine-tuning) de modelos de linguagem. Ele pega um arquivo de entrada chamado trn.json e o transforma em um novo arquivo, data.json, que servirá como o dataset final."
      ],
      "metadata": {
        "id": "i9hKYjeu0Kra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "#Arquivo de entrada(Dados brutos em JSON)\n",
        "raw_file_path = '/content/drive/MyDrive/TechC3/trnF.json'\n",
        "\n",
        "#Config\n",
        "INSTRUCTION = \"Describe book as accurately as possible.\"\n",
        "QUESTION = \"Could you give me a description of the book '{}'?\"\n",
        "MAX_RECORDS = 1000000\n",
        "counter = 0\n",
        "\n",
        "final_data = []\n",
        "\n",
        "#Leitura e processamento do arquivo bruto\n",
        "with open(raw_file_path, 'r', encoding = 'utf-8') as file:\n",
        "  for row in file:\n",
        "    try:\n",
        "      json_parsed = json.loads(row)\n",
        "      title = json_parsed.get('title')\n",
        "      content = json_parsed.get('content')\n",
        "\n",
        "      if not title or not content:\n",
        "        continue\n",
        "\n",
        "      input = f'Question: {QUESTION.format(title)}'\n",
        "\n",
        "      #Gera múltiplos exemplos (variações repetidas)\n",
        "      for i in range(1, 100):\n",
        "        final_data.append(\n",
        "            {\n",
        "                \"instruction\": INSTRUCTION,\n",
        "                \"input\": input,\n",
        "                \"output\": content\n",
        "            }\n",
        "        )\n",
        "\n",
        "        counter = len(final_data)\n",
        "\n",
        "        #Exibe progresso a cada 50.000 registros\n",
        "        if counter % 50000 == 0:\n",
        "          print(f\"counter: {counter}\")\n",
        "\n",
        "        #Interrompe se atinger o limite máximo\n",
        "        if counter >= MAX_RECORDS:\n",
        "          break\n",
        "    except Exception as e:\n",
        "      print(row)\n",
        "      raise\n",
        "\n",
        "\n",
        "#Arquivo de saída (JSON formatado)\n",
        "format_data_file_path = '/content/drive/MyDrive/TechC3/data.json'\n",
        "with open(format_data_file_path, 'w') as f:\n",
        "  json.dump(final_data, f, indent=2)\n"
      ],
      "metadata": {
        "id": "E3SHmdn20Z_A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatação do Dataset para Fine_Tuning\n",
        "\n",
        "Neste trecho está sendo preparado o dataset bruto para ser utilizado no treinamento supervisionado de modelos de linguagem."
      ],
      "metadata": {
        "id": "j0LUM4RJE0KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importa a função load_dataset da biblioteca datasets (Hugging Face)\n",
        "#Usada para carregar o dataset JSON.\n",
        "from datasets import load_dataset\n",
        "\n",
        "#Define o template (prompt) no estilo Alpaca.\n",
        "#Esse formato organiza as intruções, entradas e saídas de forma padronizada.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Define o token de fim de sequência do tokenizer (vai ser adicionado ao final do texto).\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "#Função para formatar os exemplos do dataset no padrão Alpaca.\n",
        "def formatting_prompts_func(examples):\n",
        "  #Extrai as colunas relevantes do dataset (cada uma é uma lista).\n",
        "  instructions = examples['instruction']\n",
        "  inputs = examples['input']\n",
        "  outputs = examples['output']\n",
        "  texts = []\n",
        "\n",
        "  #Junta os calores de instruction, input e output em um unico texto, aplicando o templete definido acima.\n",
        "  for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "    text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "\n",
        "  #Retorna no formato esperando pelo Hugging Face Datasets (coluna \"text\").\n",
        "  return {'text': texts}\n",
        "\n",
        "#Carrega o dataset salvo no Google Drive (aquele \"final_data.json\" gerado antes).\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files='/content/drive/MyDrive/TechC3/data.json',\n",
        "    split='train'\n",
        ")\n",
        "\n",
        "#Aplica a função de formatação para transformar o dataset no formato pronto para treinamento (instrução + input + resposta).\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "WzihWr8tFGDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O resultado é um dataset unificado e pronto para ser usado no treinamento de modelos de linguagem adaptados via LoRA ou SFT(Supervised Fine-Tuning)\n",
        "\n",
        "\n",
        "Configuração do Fine-Tuning com LoRA e SFTTrainer\n",
        "\n",
        "Este trecho do código adapta um modelo de linguagem utilizando a técnica LoRA (Low-Rank Adaptation) para reduzir o custo de treinamento e otimizar o uso de memória. Em seguida, cria um objeto SFTTrainer (Supervised Fine-Tuning Trainer), responsável por executar o processo de ajuste fino supervisionado no dataset formatado previamente."
      ],
      "metadata": {
        "id": "q4wYQ1CXtdvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "#Adapta o modelo para LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    lora_alpha = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\"\n",
        ")\n",
        "\n",
        "\n",
        "#Define o treinador para fine-tuning supervisionado\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,        #Tamanho do batch por GPU\n",
        "        gradient_accumulation_steps = 4,        #Acumula gradientes antes de atualizar os pesos\n",
        "        warmup_steps = 5,                       #Passos de aquecimento do LR\n",
        "        max_steps = 100,                        #Número total de steps de treino\n",
        "        learning_rate = 2e-4,                   #Taxa de aprendizado inicial\n",
        "        fp16 = not is_bfloat16_supported(),     #Usa FP16 se não houver suporte ao bfloat16\n",
        "        bf16 = is_bfloat16_supported(),         #Usa bfloat16 se suportado\n",
        "        logging_steps = 1,                      #Frequência de logs\n",
        "        optim = \"adamw_8bit\",                   #Otimizador eficiente em memória\n",
        "        weight_decay = 0.01,                    #Penalização para evitar overfitting\n",
        "        lr_scheduler_type = \"linear\",           #Schedule linear para LR\n",
        "        seed = 7043,                            #Semente para reprodutibilidade\n",
        "        output_dir = \"outputs\",                 #Onde salvar checkpoints/resultados\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "Ucs-wmxAuS8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saída**\n",
        "\n",
        "Este Trecho salva no Google Drive o modelo ajustado com LoRA e o tokenizer utilizado no treinamento."
      ],
      "metadata": {
        "id": "VXqmsnbzwanZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/TechC3/lora_dataF\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/TechC3/lora_dataF\")"
      ],
      "metadata": {
        "id": "q3MebRrZxp7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carregamento do Modelo Base com Adaptador LoRA para inferência**\n",
        "\n",
        "Este trecho de código carrega o modelo LLaMA-3 em 4 bits utilizando a biblioteca Unsloth, e aplica o adaptador LoRA previamente treinado e salvo no Google Drive."
      ],
      "metadata": {
        "id": "6-HzhiQtyBq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/TechC3/lora_dataF"
      ],
      "metadata": {
        "id": "WKh-AzEuylTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultado: Geração de Texto com Pipeline**\n",
        "\n",
        "Este trecho de código utiliza a biblioteca Transformers para facilitar a inferência por meio do pipeline de geração de texto."
      ],
      "metadata": {
        "id": "i5-z9wlNzp56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importa a biblioteca de pipeline para facilitar\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "#Instrução e pergunta\n",
        "INSTRUCTION = \"Describe book as accurately as possible.\"\n",
        "QUESTION = \"Question: Could you give me a description of the book '{}'?\"\n",
        "book_title = \"Clean Architeture: A Craftsman's Guide to Software Structure and Design\"\n",
        "\n",
        "#Template no formato Alpaca\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Prepara o prompt final\n",
        "final_prompt = alpaca_prompt.format(\n",
        "    INSTRUCTION,\n",
        "    QUESTION.format(book_title),\n",
        "    \"\"\n",
        ")\n",
        "\n",
        "#Cria o pipeline de geração de texto\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    torch_dtype = torch.float16,\n",
        "    device_map = \"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "#Gera a saída do modelo\n",
        "outputs = pipe(\n",
        "    final_prompt,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "#Exibe o texto gerado\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "lVoQHrzXz6pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusão**\n",
        "\n",
        "Neste notebook, realizamos todo o fluxo de fine-tuning supervisionado utilizando a técnica LoRA (Low-Rank Adaptation) em um modelo LLaMA-3 quantizado em 4 bits, com o apoio da biblioteca Unsloth para maior eficiência.\n",
        "\n",
        "As etapas desenvolvidas incluíram:\n",
        "\n",
        "1. **Configuração do ambiente** e instalação das dependências necessárias.\n",
        "\n",
        "2. **Pré-processamento e formatação do dataset**, transformando dados brutos no padrão Alpaca (Instruction, Input e Response).\n",
        "\n",
        "3. **Adaptação do modelo com LoRA**, permitindo treinar de forma eficiente mesmo em recursos computacionais limitados.\n",
        "\n",
        "4. **Treinamento supervisionado** com parâmetros otimizados para balancear custo computacional e qualidade do ajuste.\n",
        "\n",
        "5. **Salvamento e recarregamento** do modelo e tokenizer, garantindo reuso sem necessidade de retreinar.\n",
        "\n",
        "6. **Inferência com pipeline** de geração de texto, validando a capacidade do modelo em responder a prompts personalizados.\n",
        "\n",
        "Com isso, foi possível compreender e aplicar na prática o processo de ajuste fino de modelos de linguagem para um caso de uso específico (descrição de livros). O pipeline pode ser adaptado para outros domínios de conhecimento, bastando alterar o dataset de entrada e as instruções fornecidas."
      ],
      "metadata": {
        "id": "Y9laHK534pkE"
      }
    }
  ]
}